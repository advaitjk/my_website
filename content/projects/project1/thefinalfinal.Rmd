---
title: "Session 2: Homework 1"
author: "Group 9: Lexin Xu, Ozlem Cuhaci, Advait Jayant, Rasul Rasulov, Joseph Perrin"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
# We start by downloading all the packages necessary for this homework
library(tidyverse) 
library(ggrepel)
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(tidyquant)
library(vroom)
library(kableExtra)
library(knitr)
library(tidytext)
library(ggpubr)

```

# Where do people drink the most beer, wine, and spirits?

As international students starting a master in one of the most diverse universities worldwide, a global alcohol consumption analysis is definitely required to impress our fellow MIM and MFA friends at parties. After all, we all know that alcohol is the secret ingredient to developing deep and genuine connections. 
We begin our process by loading the dataset! 

```{r, load_alcohol_data}
# Loading the alcohol data

library(fivethirtyeight)
data(drinks)

# display data
drinks

```
We will first use the glimpse function to see what the dataset looks like.

```{r, glimpse_data}
glimpse(drinks)
```

Through the glimpse of the data we can see that this dataset contains information about beer servings, wine servings, spirit servings, and total alcohol consumption, for each country.

We then use the skim function to obtain information on the 5 variables present in the data set. The table shows us that there are no missing values, therefore the the completeness rate for each variable is of 1. 

```{r glimpse_skim_data}

skim(drinks)

```

A skim of the dataset shows that our dataset contains information about alcohol consumption in 193 countries. Through skimming the dataset, we can clearly see that there are no missing values in the dataset. Now, that we understand the structure of the data, it is time for us to actually begin the fun part - the analysis!!
\n
We start this analysis by observing the global beer consumption measured as number of servings per capita. We filter the data to have the "Country" and "Beer Servings" columns independent before selecting the top 25 countries in descending order. Next, we plot the findings with Countries in the Y axis and Beer servings on the X axis. 

```{r beer_plot}
# Filter the data frame to countries and beer servings

drinks%>% 
  select(country,beer_servings) %>% 
  arrange(desc(beer_servings)) %>% 
  head(25) %>% 
  
# Create a plot to show the most consuming countries
  ggplot(aes(y= reorder(country, beer_servings), x= beer_servings))+
  geom_col()+
  
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (15)),
        axis.title = element_text(family = "Helvetica", size = (10)),
        axis.text = element_text(family = "Helvetica", size = (10)))+
  ggtitle("Beer Servings per Country")+
  labs(y= "Country", x= "Number of Servings", caption="SOURCE: FiveThirtyEight Alcohol Consumption Dataset")+
  theme_minimal()

```

Similarly to the beer servings graph, we start by filtering the data to have the "Country" and "Wine Servings" columns independent before selecting the top 25 countries in descending order. We plot the findings with Countries in the Y axis and Wine servings on the X axis. We will be placing our bets on France of course, as one of the authors is from France. 

```{r wine_plot}

# Filter the data frame to countries and wine servings
drinks%>% 
  select(country,wine_servings) %>% 
  arrange(desc(wine_servings)) %>% 
  head(25) %>% 
  
# Create a plot to show the most consuming countries
  ggplot(aes(y= reorder(country, wine_servings), x= wine_servings))+
  geom_col()+
# Let us add some style !
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (15)),
        axis.title = element_text(family = "Helvetica", size = (10)),
        axis.text = element_text(family = "Helvetica", size = (10)))+
  ggtitle("Wine Servings per Country")+
  labs(y= "Country", x= "Number of Servings", caption="SOURCE: FiveThirtyEight Alcohol Consumption Dataset")+
  theme_minimal()
```

Finally, we create a plot that displays the top 25 spirit consuming countries, using the same process as the two preceding analysis. Once again, we filter the data to have the "Country" and "Spirits Servings" columns independent before selecting the top 25 countries in descending order. We plot the findings with Countries in the Y axis and Spirits servings on the X axis.  

```{r spirit_plot}

# Filter the data frame to countries and spirits servings
drinks%>% 
  select(country,spirit_servings) %>% 
  arrange(desc(spirit_servings)) %>% 
  head(25) %>% 
  
# Create a plot to show the most consuming countries
  ggplot(aes(y= reorder(country, spirit_servings), x= spirit_servings))+
  geom_col()+

  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (15)),
        axis.title = element_text(family = "Helvetica", size = (10)),
        axis.text = element_text(family = "Helvetica", size = (10)))+
  ggtitle("Most Spirits Servings per Country")+
  labs(y= "Country", x= "Number of Servings", caption="SOURCE: FiveThirtyEight Alcohol Consumption Dataset")+
  theme_minimal()

```

Let us compare the three above graphs by going through the most servings per capita for wine, beer and spirits. 

Quite interestingly, the most consuming countries for both beer and wine include major producers. For instance, France is the country with most wine servings per capita, and the rest of the top 25 is composed of Italy, Portugal, Chile, Argentina and United States. The same goes for beer, with Czech Republic being the second most consuming countries based on servings per capita, followed by Germany, Poland, Netherlands and Belgium.

We guess every country follows the "Follow what you really love" motto when it comes to making their favourite beverages. 
Now surprisingly, the country with most beer servings per capita is Namibia, standing at 376 servings per year/capita!!! This is definitely going to be one of the most interesting facts that we tell our friends at parties. Upon seeing this, we felt quite stunned and decided to conduct a little more of our own research into this matter, because anyone would have placed their bet on Germany (A google search shows Oktoberfest when we google "beer festival"?!!!!). So we actually ended up finding that Namibia is old colony of Germany, a country well known for its beer culture. So that's probably where this habit comes from! 

Moreover, the most spirits servings per capital per country graph is the one with most striking results. The ranking is shared among "white" (alcohol poor in congeners such as vodka, gin, white rum, rice alcohols) and "brown" (alcohol poor in congeners such as whiskey, dark rum, brandy and tequila). As a matter of fact, we can observe a large presence of eastern European countries (Belarus, Bulgaria, Ukraine, Poland) and Russia which are known for being vodka consumers. 

On the other hand, Central American countries (Grenada, Haiti, Santa Lucia, St Vincent and the Grenadines) are countries consuming much dark rum. Finally, this graph shows that countries with most extreme weather conditions are the ones consuming the most spirits - be it extreme heat, Rum in the Carribeans, or Vodka in the Russian Federation. 


# Analysis of movies- IMDB dataset

Now that we know our alcohol, we will now take a look at what bonds people even closer together - movies!!! 
In order to know about all the possible popular and good movies, we thankfully have IMDB, therefore, we will be using the IMDB 5000 dataset to perform our analysis. 

Whenever approaching a problem statement, we need to make sure to use the IICE 4 step plan.  

```{r load_movies, warning=FALSE, message=FALSE}

# We will load our datasets using read_csv function and use glimpse to take a look at the dataset.
movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)

```

We can see that this dataframe includes certain details about each movie. These are their title, genre, director, release year, duration, gross earning, budget, the number of facebook likes cast members received and the number of IMDb votes, reviews and rating. Now let's see what we can actually learn from this dataset - gotta impress our peers right? 

The New York Times conducting an analysis and found that most data scientists actually spend 70% of their time cleaning the data and ensuring that it is nearly perfect for evaluating their predictions. So before we actually get to the fun part, as budding data scientists, it is our responsibility to perform data cleaning and data wrangling to ensure that we are using the most optimized subset of the dataset to structure our hypothesis. 

```{r gross_on_budget}
#Skim function provides a broad overview of the dataframe for us whereas duplicated shows us if there are any duplicated values in the dataframe.
skim(movies)
sum(duplicated(movies))
```

We can see that there are no missing values in any of the attributes. If there were any duplicate entries then they would be marked true in `duplicate(movies)`. The sum `sum(duplicated(movies))` gives us 0. And as we know that true means 1, we do not have any duplicate entries.

But wait a minute!!! There is something fishy definitely going on here behind the scenes. We can see that there are 2,961 records in this dataframe but there are only 2907 unique titles?! We now need to check whether there is actually some fishy title business or not.

```{r movies_title_duplicated}
#We will look at only the movie titles now whether they have duplicates or not.
sum(duplicated(movies$title))

#Let us take a look at all the duplicated movies
movies %>%
  #We will filter the movies which have duplicates
  filter(duplicated(movies$title)) %>% 
  #We will arrange them by title in an alphabetical order
  arrange(title) %>% 
  #We will only display 5 of the duplicates
  top_n(5) %>%
  #We should remove the duplicate movies %>%
  kable(caption="Duplicated Movies", col.names=c("Movie Title", "Genre", "Director", "Year", "Duration", "Gross earnings", "Budget", "Facebook likes", "Votes", "Reviews", "Rating")) %>% 
  #Why not add some styling? 
  kable_styling()
```

We can see that there are 54 movies having the same title. The skim function also said that there are 2907 unique titles and 2961 total movies in the list. So we have 54 again. When we filtered the duplicate movie titles, we saw the top 5 of them that appear twice in our dataframe. 

Now let us actually remove all of these duplicates! The duplicates differed in either the number of reviews or the number of votes and therefore could not be spotted if looking at only whether there exist duplicates or not in the dataframe.

```{r removing_duplicates, echo=FALSE}
movies <- movies %>% 
  filter(!duplicated(movies$title)) 
```
Now that we have done the initial data cleaning process and 70% of our jobs as Data Scientists, let us take a look at all the movies present in this database and try to understand how many movies are contained for each genre. So, we will construct a table with the count of movies by genre, ranked in descending order. Now that our data is clean, we need to start our exploration process (as elaborately put by the IICE methodology).


```{r movies_by_genre}

# This gives us a table which shows that Comedy, Action and Drama genres have the most number of movies in the database.

movies %>% 
  #We will group the movies by genre
  group_by(genre) %>% 
  #Now we count the number of movies per genre and rank them in descending order
  count(sort=TRUE) %>% 
  #We add a title to the table and give names to its columns.
  kable(caption="Number of movies per genre", col.names=c("Genre", "Number of movies")) %>% 
  #Why not add some styling?
  kable_styling()

```

We can see that this database actually contains a majority of Comedy, Action, Drama, Adventure, Crime, Biography, and Horror movies. 

## NOTE to Prof. Kostis

In the IMDB Dataset, there exist some genres that have lesser than 10 observations in them. Most of our analysis in this segment of the Homework is centered on finding the correlations between genres and several different attributes such as average ratings, gross budget, return on investment etc. Most of these genre classes contain more than 10 values but there are 6 classes which contain less than 10 values but bias our analysis plots. We were unsure if we are allowed to remove them completely from our analysis tables and plots or not, so we have kept them but mentioned that there are very little values in these classes so therefore it's best to take them with a grain of salt.

## Back to Analysis

Now, we will look at the average gross earnings and budget by genre plot. Using this, we will also calculate the return on budget that a movie made at the box office for each dollar.
This is going to be really interesting because this will allow us to see if there is indeed a direct correlation between a movie's budget and its return! As MAM students, this is definitely the kind of analysis that we want to be performing while beginning any entrepreneurial journey!

```{r return_on_budget}
movies %>% 
  #We will group movies by genre
  group_by(genre) %>% 
  #We will calculate the average gross earning and budget in millions and turn these observations into a single data point per genre.
  summarize(avg_gross = mean(gross)/1000000, 
            avg_budget= mean(budget)/1000000 ) %>% 
  #We will add another column named return_on_budget and calculate it.
  mutate(return_on_budget = 100* (avg_gross - avg_budget) / avg_budget) %>% 
  #Now, arrange it by return on budget in descending order
  arrange(desc(return_on_budget)) %>% 
  #Again, add names to the table and columns
  kable(position="center", caption="Average USD gross earning, average USD budget and return on budget per genre", col.names=c("Genre", "Average gross earning(m)", "Average budget(m)", "Return on budget(%)")) %>% 
  #Add some styling to the table
  kable_styling()

```

At first glance at this plot, we thought about entering into the Musical industry! The average return on investment is nearly 27 times the initial budget which is insane! However, we remembered that there are only two Musical movies in this database unfortunately, so it's too little to predict an accurate return on our investment since we clearly don't have enough data. 

Now, let's produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. We will also show the mean, median and standard deviation per director. The box office is one of those places where the amount of money earned is directly dependent upon the people reached. So, now, we will try to analyse the list of directors who have touched the maximum number of lives through their movies! 

```{r gross_on_fblikes}
movies %>% 
  #We group movies by director
  group_by(director) %>% 
  #We will calculate the total gross earning in billions and mean, median and standard deviation in millions and summarize them per director.
  summarize(sum_gross=sum(gross)/1000000,   
            mean = mean(gross)/1000000,               
            median = median(gross)/1000000,
            sd = sd(gross)/1000000 ) %>% 
  #We will arrange them by total gross earnings in descending order
  arrange(desc(sum_gross)) %>%   
  #We need to show only the top 15 directors
  head(15) %>% 
  #Again, add names to the table and columns
  kable(caption="Top 15 directors by highest revenue in USD", col.names=c("Director", "Total gross Earning(m)", "Mean(m)", "Median(m)", "SD(m)")) %>% 
  #Add some styling to the table
  kable_styling()


```

Okay so the data we are dealing with is definitely accurate since these 15 directors are absolute geniuses. 

Now, we come to the part to learn what the audience think about these movies. We will produce a table that describes how ratings are distributed by genre. We know that money is definitely one way an audience shows love to the movie, but a movie can actually perform incredibly at the box office even if it isn't a great movie - it all depends upon how good the marketing geniuses behind the film's promotion are good at their jobs. In today's day and age, there exist very few people who watch a movie without checking for its review in IMDB, so now let's see how the ratings of movies are distributed by their genre. After all, we still do want to confirm which movies to make as making a movie that earns money but doesn't get us the people's love isn't really fun, is it? 


```{r ratings_genre_dataframe}
movies %>% 
  #Group movies by genre
  group_by(genre) %>% 
  #Summarize mean, min, max, sd of ratings per genre
  summarise(mean= mean(rating), 
            min=min(rating),
            max=max(rating),
            sd=sd(rating)) %>% 
  #Just arrange them to make the table more readable
  arrange(desc(mean)) %>%
  #Add table title  and column names
  kable(caption="Distribution of ratings by genre", col.names = c("Genre", "Mean", "Min", "Max", "SD")) %>% 
  #Add some styling to the table
  kable_styling()

  
```

Now, we can see that the audience is particularly fond of watching biographies, crime, mystery, and drama movies. 

``` {r ratings_genre_plot}
  #Plot a histogram using movies dataframe, x axis shows rating
  ggplot(data = movies, mapping = aes(x = rating) ) +
  #Choose white for color of bin lines
      geom_histogram(color="white") +
  #Add title for the plot and x and y axis
      labs(title="Distribution of ratings", x="Rating", y="Number of movies", caption="SOURCE: IMDB 5000 Movie Dataset") +
  #Choose the theme minimal for the plot styling
      theme_minimal() 

```

This histogram clearly shows that most movies tend to obtain IMDB ratings between 5 and 7.5. 

Now, we will examine the relationships between some variables for the next few analysis. Firstly, we will produce a scatter plot to see whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. 

```{r gross_on_ggplot}


#We will plot a scatterplot with movies data, we will map the number of facebook likes that the cast received in thousands on the x axis and gross earnings in millions on the y axis.
ggplot(data = movies, mapping = aes(x=cast_facebook_likes/1000, y=gross/1000000)) +
      geom_point() + 
      #We will add a title to the plot and to the x and y axis.
      labs(title="Relationship between gross revenue and facebook likes of a movie", x="Number of facebook likes(k)", y="Gross earning(m USD)", caption="SOURCE: IMDB 5000 Movie Dataset") +
      #We will choose theme minimal for the plot styling.
      theme_minimal()   


```

This graph depicts that there doesn't exist any significant correlation between the Number of Facebook Likes and Gross Earnings. But we aren't 100% sure if this is actually the case at this point, let us try to rearrange our graph to see if this actually holds true! 

```{r gross_on_ggplot2}
# We will create another version of this scatter plot
ggplot(data = movies, aes(x=cast_facebook_likes/1000, y=gross/1000000)) + 
  # We will set the opacity of the points to 0.2
  geom_point(alpha = 0.3) + 
  # We will try to depict the smoothing line
  geom_smooth() + 
  # We will try to get a logarithmic scale for both these axis
  scale_y_log10() + 
  scale_x_log10() +
  # Finally we will label our plot again
  labs(title = "box office earnings on number of facebook likes",
       subtitle = "logarithmic scales",
       x = "Number of facebook likes(k)",
       y = "Gross earning(m USD)",
       caption="SOURCE: IMDB 5000 Movie Dataset")
```

From this plot, we can actually see that there exists a correlation between the gross earnings and the Number of Facebook Likes! So this means that while producing a movie, we should definitely consider allocating some budget to marketing the movie on Facebook through the best possible way! 

Now let us take a look at a scatter plot between the budget of the film and its gross revenues at the box office. We want to see if we really do need to spend a lot of money out of our pockets while creating the film to ensure that it performs well at the box office! 

```{r gross_budget_scatter}

#We will plot a scatterplot with movies data, we will map the gross earnings on the x axis and budget on the y axis, both are in millions
ggplot(data = movies, mapping = aes(x=gross/1000000, y=budget/1000000)) +
      geom_point() + 
#We will draw a line of best fit to better understand the trend
      geom_smooth() +
#We will add titles
      labs(title="Relationship between gross revenue and budget of a movie", x="Gross revenue(m USD)", y="Budget(m USD)", caption="SOURCE: IMDB 5000 Movie Dataset") +
#Choose theme minimal for plot syling
      theme_minimal()       

```

There exists a clear correlation between budget and gross that can be modeled by the polynomial plot, therefore it is likely that budget is a good predictor of how much money a movie will make at the box office. This means that while making our own movies, we will need to guarantee that we have at least a respectable few millions to make the movie. 

Next, we will take a look at the scatterplot faceted by genre to see whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Let’s see if there is anything strange in this dataset.

```{r gross_on_rating}

#We will produce a scatterplot with movies data, we will map IMDb ratings on the x axis and gross earnings in millions on the y axis.
ggplot(data = movies, mapping = aes(x=rating, y=gross/1000000)) +
      geom_point(alpha=0.2) +
      #We will produce a plot for each genre
      facet_wrap(~genre) +
      #Adding a fitting line using the geom_smooth function
      geom_smooth() +
      #Add title for plot and axis
      labs(title="Relationship between gross revenue and rating by genre", x="Rating", y="Gross revenue(m USD)", caption="SOURCE: IMDB 5000 Movie Dataset") 

```

Action and Adventure movies with high IMDb ratings have gained better box office revenue so you might think ratings are a good predictor of the box office earnings. But this trend changes according to the genre, we see that for example gross revenue for Comedy, Crime and Drama genres are not sensitive to rating and follow a straight line. And finally, for Musical, Romance and Western movies we don't have much data to make an inference. Therefore, IMDB ratings are not always an indicator of how much a movie will make.

In this part of the project, we were able to ascertain a few important points:

- There exists a positive correlation between the Gross Earnings of a movie and the number of Facebook likes obtained; this means that it is a good idea to market the movie on Facebook extensively pre-release.
- There exists a positive correlation between a film's budget to its gross earnings at the post office; so while looking at producing a movie, it is generally a good idea to put in some money.
- The top 15 Directors are (undoubtably) Steven Spielberg, Michael Bay, James Cameron, Christopher Nolan, George Lucas, Robert Zemeckis, Tim Burton, Sam Raimi, Clint Eastwood, Francis Lawrence, Ron Howard, Gore Verbinski, Andrew Adamson, Shawn Levy, and Ridley Scott.



# Returns of financial stocks

For the purpose of this analysis, we have downloaded the `tidyquant` package which allows us to download historical data of stock prices, calculate returns, and examine the distribution of returns. 
Before diving into our analysis, let's put a bit of context to this exercice. As 5 new students from LBS, we (Lexin, Ozlem, Advait, Rasul and Joseph) have decided to pool our financial resources to discover the intriguing world of finance to invest in US stocks. 

We will now use our analytical skills to assess the quality of our investments. Let's start by loading the data 


```{r load_nyse_data, message=FALSE, warning=FALSE}
#read the Nyse CSV file
nyse <- read_csv(here::here("data","nyse.csv")) 
```

We will start by analysing the number of companies we have per sector. 

```{r companies_per_sector}
#We start by formating the data to have sectors and number of companies aligned. 
nyse_by_sector <- nyse %>% 
  #Grouping by sectors
  group_by(sector) %>% 
  #Counting the sectors
  count(sector, sort = TRUE)

nyse_by_sector
```

Now let us try to actually visualise this data into a plot.

```{r nyse_per_sector_plot}
#We create a plot to see how many companies we have per sector in descending order.  
ggplot(nyse_by_sector, aes(y = reorder(sector,n), x=n))+
  geom_bar(stat = "identity")+
  theme_minimal()
```


As we all have very different interest, we selected companies that we liked in several industries. We chose to invest in the following stocks: Apple (AAPL), Hewlett Packard (HPE), Nike (NKE), Disney (DIS), Delta Airlines (DAL), Ford (F) and Tesla (TSLA). 
In order to benchmark our investments with the rest of the market, we include SP500 ETF (Exchange Traded Fund) (SPY) to our analysis.

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
#We extract the data of the selected stocks between January 2011 till August 2020. 

myStocks <- c("AAPL","HPE", "NKE","DIS","DAL","TSLA","F","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2020-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) 
```

The next step of this analysis is to collect information on returns for each of the aforementioned stocks. We collect return information on daily, monthly and yearly basis. 

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

We then create a table to quickly observe which of the selected stocks have the best monthly returns, minimun, maximum, median and risk (standard deviation). 

```{r summarise_monthly_returns}
#We create a table grouped by symbol, and calculate mean, max, min, median and standard deviation for monthly returns. 

summary_month<- myStocks_returns_monthly %>% 
  group_by(symbol) %>%
  summarise(min = min(monthly_returns), max=max(monthly_returns), median=median(monthly_returns), mean=mean(monthly_returns),                 sd=sd(monthly_returns)) %>% 
  kable(caption="Summary of Monthly Returns", col.names = c("Symbol", "Min", "Max", "Median", "Mean", "SD")) %>% 
  kable_styling()
 
summary_month
```

Now, we create a density plot to observe the average monthly return of our selected stocks over the 9 year period. 

```{r density_monthly_returns}

myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  ggplot(aes(x=monthly_returns, color =symbol))+
  geom_density()+
  facet_wrap(~symbol, scales="free_x")+
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (15)),
        axis.title = element_text(family = "Helvetica", size = (10)),
        axis.text = element_text(family = "Helvetica", size = (10)))+
  ggtitle("Variability of Monthly Returns")+
  labs(y= "Density", x= "Monthly Returns", caption="SOURCE: NYSE Dataset")+
  theme_minimal()


```

From this graph, it appears that all the stocks selected for this analysis are more risky than the "Overall Market", which is represented by SPY (SP500 ETF (Exchange Traded Fund)). In fact, we cant see that the majority of the monthly returns of SPY are positive, which means that the overall stocks grew in the analysed period of time. 
As a risk taking group in this period of economic recession, we have decided to place our savings on risky stocks that could greatly increase our returns. However, several of our stocks regularly display negative returns. This is the case for Hewlett Packard which has slightly more negative monthly returns than other stocks. Other risky stocks are Apple (AAPL) or Delta Airlines (DAL), which have a higher density of negative monthly returns than Nike (NKE) over a single month. 


In order to bring our analysis one step forward, we now plot our expected returns (calculated as the mean of monthly returns over the studied period of time) with expected risk (calculated as the standard deviation of the monthly returns over the same period of time). 

Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}
#Assign mean and sd of expected return to a new variable
expected_monthly_returns <- myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  summarise(mean=mean(monthly_returns), sd=sd(monthly_returns))



expected_monthly_returns%>% 
  ggplot(aes(y=mean,x= sd, color= symbol))+
  geom_point()+
  geom_text_repel(aes(label = symbol))+
  labs(title="Risk and Return Analysis",y= "Returns", x= "Risk", caption="SOURCE: NYSE Dataset")+
  theme(plot.title = element_text(family = "Helvetica", face = "bold", size = (15)),
       axis.title = element_text(family = "Helvetica", size = (10)),
       axis.text = element_text(family = "Helvetica", size = (10)))+
  theme_minimal()

```

From this analysis, it appears that our investment selection is very consistent with regards to the ration between expected returns and risk, except for Tesla, which has both very high risks and very high returns. All the other stocks we selected have a very similar risk to expected return ration. 

The worst stock we have is Ford, which has a relatively high risks but which displays negative expected returns. This situation indicates that this we an negative asymmetrical return situation, meaning that we have more to lose than win with this stock. 

On the other hand, Apple stocks entails quite a high risk as well, but displays very positive returns (over 0.015 points over the SP500). Similarly, Nike (NKE) has quite high returns associated with the lowest risk of our stock selection. Hopefully our strategy will pay off !


# The IBM HR Analytics

The objective of this task is to analyze the [IBM HR Analytics Employee Attrition & Performance data set], which is a fictional data set created by IBM data scientists. This data set contains a variety of personal and professional information regarding employees, let us dive into it!


First we will load the data

```{r}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
glimpse(hr_dataset)

```

We will clean the data set, as variable names are in capital letters, some variables are not really necessary, and some variables, e.g., `education` are given as a number rather than a more useful description

```{r}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)
 
```


First, we will take a look at attrition and find out how often do people leave the company. After dividing the count of people who leave the company of total number, we can see that 16.1% of people leave the company. 

```{r}

hr_cleaned%>% 
  summarise(count(attrition=="Yes")/n()*100) %>%   #Calculate percentage
  kable(caption="Attrition Rate", col.names = c("Attrition Rate (%)")) %>%   #Output table
  kable_styling()

```


Then, we want to take a look at how `age`, `years_at_company`, `monthly_income` and `years_since_last_promotion` are distributed. 

However, before visualize the distribution of these variables, we can roughly guess which one is closer to Normal Distribution just by looking at summary statistics. As shown below, the mean and median of `age` are closer compared with other variables. Also, the gap between 3rd quartile and maximum is dramatically bigger with `years_at_company`, `monthly_income` and `years_since_last_promotion`, while for age, the gap between minimum and 1st quartile is similar to that of 3rd quartile and maximum. So we may roughly guess that `age` is closer to Normal Distribution (which is in line with common sense).

After using 'geom_density' to visualize the distribution, we can see that `age` is closer to Normal distribution, while `years_at_company`, `monthly_income` and `years_since_last_promotion`are skewed to the right. Our assumption has been proven to be correct!

```{r}
hr_new=hr_cleaned%>%   #Create hr_new with 4 key variables, then summarize it
  select(age, years_at_company, monthly_income, years_since_last_promotion)
summary(hr_new)


library(ggpubr)   #Load "ggpubr", so that we can use the ggarrange function to consolidate 4 ggplots on 1 single page, which makes it easier for readers to see

p1 <-  ggplot(data = hr_cleaned, mapping = aes(x = age) ) +   #Have a density plot separately
           geom_density() +
           labs(title="Distribution of age", x="Age", caption="SOURCE: IBM HR Dataset") +
           theme_minimal() 
p2 <- ggplot(data = hr_cleaned, mapping = aes(x = years_at_company) ) +
          geom_density() +
          labs(title="Distribution of `years_at_company`", x="`years_at_company`", caption="SOURCE: IBM HR Dataset") +
          theme_minimal() 
p3 <- ggplot(data = hr_cleaned, mapping = aes(x = monthly_income) ) +
          geom_density() +
          labs(title="Distribution of `monthly_income`", x="`monthly_income`", caption="SOURCE: IBM HR Dataset") +
          theme_minimal() 
p4 <- ggplot(data = hr_cleaned, mapping = aes(x = years_since_last_promotion) ) +
          geom_density() +
          labs(title="Distribution of `WorkLifeBalance`", x="`years_since_last_promotion`", caption="SOURCE: IBM HR Dataset") +
          theme_minimal() 

ggarrange(p1, p2, p3, p4,   #consolidate 4 ggplots on 1 single page to make it clear to read
          ncol = 2, nrow = 2)

```


After that, we will look into the distribution of `job_satisfaction` and `work_life_balance`. We use "geom_bar" to visualize employees' opinions on job satisfaction and work-life balance. It is shown that most people have high job satisfaction, and the majority of employees claim to have better work-life balance in the company.

However, if we put job satisfaction and work-life balance together, we can see that despite different work-life balance level, job satisfaction distribution has only minor variations. This means that there seems to be no strong relationship between job satisfaction and work-life balance.

```{r}

#Create 2 bar plots for job satisfaction and work-life balance
g1 <- ggplot(data = hr_cleaned, 
             mapping = aes(x = factor(job_satisfaction, levels = c("Low", "Medium", "High", "Very High")))) + #Change into correct order
            geom_bar() +
            labs(title="Distribution of Job Satisfaction", x="Job Satisfaction", y="Number of people", caption="SOURCE: IBM HR Dataset") +
            theme_minimal() 
g2 <- ggplot(data = hr_cleaned, 
             mapping = aes(x = factor(work_life_balance, levels = c("Bad", "Good", "Better", "Best")))) +
            geom_bar() +
            labs(title="Distribution of Work Life Balance", x="Work Life Balance", y="Number of people", caption="SOURCE: IBM HR Dataset") +
            theme_minimal() 
ggarrange(g1, g2, 
          ncol = 2, nrow = 1)


# View the percentage of work-life balance by job satisfaction
hr_cleaned%>%
ggplot(hr_cleaned, 
       mapping = aes(x = factor(job_satisfaction, levels = c("Low", "Medium", "High", "Very High")),  #Reorder
                     fill = factor(work_life_balance, levels = c("Bad", "Good", "Better", "Best")))) +
    geom_bar(position = "fill",color="white") +
    labs(x = "Job Satisfaction",
         y = "Percentage",
         fill = "Work-life Balance",
         title = "Job Satisfaction and Work-life Balance", 
         caption="SOURCE: IBM HR Dataset") +
    theme_minimal()

```


Moving on, we are going to focus on income and what may influence it.

First off, gender. What is the relationship between monthly income and gender? Contrary to what people might think, in IBM, women have higher median monthly income than men.


```{r}

#Create a bocplot to look into the relationship between monthly income and gender
hr_cleaned%>%
ggplot(data = hr_cleaned, mapping = aes(x=monthly_income, y=gender)) +
          geom_boxplot() +
          labs(title="Relationship between monthly income and gender", x="Monthly Income", y="Gender") +
          theme_minimal() 

```


Then, let us take a look at the relationship between monthly income and education. We can see that with high education level, it is more likely that one will get a high salary.

```{r}

#Create a bocplot to look into the relationship between monthly income and education level
ggplot(data = hr_cleaned, 
       mapping = aes(x=monthly_income, 
                     y=factor(education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor"))))+
       geom_boxplot() +
       labs(title="Relationship between monthly income and education", x="Monthly Income", y="Education") +
       theme_minimal()

```


By Calculating and plotting a bar chart of the mean income by education level, it seems that as education level goes up, the average income increases. The average monthly income of people with an education level below college is under 6000, while PHD degree holders have a mean salary of over 8000.

This distribution is left skewed so it is a good idea to use the median income instead of the mean! 

```{r}

hr_avgincome <- hr_cleaned %>%  #Create hr_avgincome
  group_by(education) %>% 
  summarise(median = median(monthly_income)) #Calculate median monthly income by education level

ggplot(hr_avgincome, 
       aes(x = factor(education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor")),  #Reorder
           y = median)) + 
    geom_col() +
    labs(x = "education level",
         y = "Median monthly income",
         title = "Average Income by Education Level", 
         caption="SOURCE: IBM HR Dataset") +
    theme_minimal()

```


Visualize distribution of income by education level, all of them are skewed to the right. This finding is actually in line with common sense and public knowledge.

```{r}

#Plot the distribution of income by education level
hr_cleaned%>% 
  group_by(education)%>% 
  ggplot(aes(x=monthly_income))+
      geom_density()+
      facet_wrap(~factor(education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor")))+   #Reorder and use a facet_wrap
      labs(x="Monthly Income", y='', title="Distribution of Income by Education Level", caption="SOURCE: IBM HR Dataset") +
      theme_minimal()   #Use a theme to make charts look better

```


Next, we want to look into the relationship between income and job role.

By plotting a boxplot of income vs job role, we can see that managers tend to have the highest median monthly income, while sales representatives have the lowest median salary.

```{r}

#Plot a boxplot of income vs job role
ggplot(hr_cleaned, aes(y = reorder(job_role, monthly_income), x = monthly_income)) +   #Reorder by monthly income
    geom_boxplot() +
    labs(x = "monthly income", y = "job role",
         title = "Monthly Income Boxplots by Job Role", 
         caption="SOURCE: IBM HR Dataset") +
    theme_minimal()

```


Last, we want to find out the relationship of age and monthly income, grouped by job role. As shown below, overall, as one ages, they salary will go up. This trend is more significant in the cases of manufacturing directors, and less so for healthcare representatives and human resources.

```{r}

#Plot income vs age, faceted by `job_role`
hr_cleaned%>% 
  group_by(job_role) %>%
  ggplot(aes(x=age, y=monthly_income))+
      geom_point()+
      geom_smooth()+
      facet_wrap(~job_role, scales="free_x")+
      labs(x="Age", y="Income", title="Relationship of Age and Income by Job Role", caption="SOURCE: IBM HR Dataset") +
      theme_minimal() 

```


# Challenge 1: Replicating a chart

We created a publication-ready plot using our `dplyr` and `ggplot2` skills. We got as close as we could to the original graph, which can be accessed by the code below.

```{r challengeLEO, echo=FALSE, out.width="90%"}
library(ggrepel)
library(RColorBrewer)
knitr::include_graphics(here::here("images", "figure3.jpeg"), error = FALSE)
```

The bubble chart that we replicated shows the position of the states in terms of the firearms related cases of homicide and suicide across white males. The bubble size is determined by a white population of the state and the color of the dot is responsible for the gun ownership category. The states with high gun ownership rates are the ones with highest suicide and homicide rates. What can also be inferred from the graph is that the states with highest suicide rates and gun ownership are those with low white population figures. Hence, on average, white male has a lower chance to be involved in homicide or a suicide if he lives in a state with large white population figures.

As to the replication of this graph, it first required some work over the dataset, as only firearms related cases should be depicted and there were some NA's all over the dataset. Correct labeling of the bubbles in the chart, painting them in accordance with Gun ownership category, and setting the size proportional to white population was followed by scaling the bubble size and correctly representing the legend. Two libraries were activated to avoid text overlapping and to set matching color scheme. Custom theme allows to control for font size, grid lines and other. Figure size was controlled for and the graph was set to be square to match the original version. 
Below you can see our version of the article plot:


```{r, challenge1, echo=TRUE, fig.width=7.5, fig.height=7.5}

# Replicate Figure 3
CDC <- read_csv(here::here("data", "CDC_Males.csv"))
CDC %>%
  filter(type=="Firearm") %>% #use this function to select only firearms cases
  drop_na("gun.house.prev.category") %>% #omitting missing values
  ggplot(mapping=aes(x=adjusted.suicide.White, #using ggplot to build a bubble plot
                     y=adjusted.homicide.White, #setting x and y
                     size=average.pop.white, #setting bubble size proportional to white population                                    
  fill=gun.house.prev.category, label=ST))+ #labeling bubbles by the states' names and painting them according to gun ownership category
  geom_point(shape=21, color="black", stroke=0.5)+ #choosing dotplot, controlling the shape by "shape" and color using "color" and outline width using "stroke" set to 0.5
  scale_size(range = c(1, 15), name = "White Population", breaks=c(500000,1500000,3000000,7000000), #range" allows to control the bubble size, "name" makes it possible to name the circle size legend and "breaks"   allow to choose the number of circles in the legend and control their size
  label=c("500k","1.5m","3m","7m"))+ #labels allow to replicate the legend values
  geom_text_repel(size=4)+ #making size of the states names similar to the ones in the article plot
  labs(fill="Gun ownership")+ #naming the legend
  ylab("White Homicide Rate (per 100 000 per Year)")+ #naming y axis
  xlab("White Suicide Rate (per 100 000 per Year)")+ #naming x axis
  scale_fill_brewer(palette = "OrRd")+ #setting color scheme as in the original version
  theme(axis.title = element_text(size=10), #axis titles edited
        axis.text = element_text(size=8, color="black"), #axis text edited
        panel.grid.major=element_line(color = "gray90", size = 0.5), #adding major grid  lines
        panel.grid.minor = element_line(color = "gray97", size=0.5), #adding minor grid lines
        legend.title=element_text(size=10), #legend title edited
        legend.text = element_text(size=9), #legend text edited
        legend.key = element_blank(), #removing legend key background
        panel.background = element_rect(color="black", fill="white"))+ #graph background set to white and contour set to black
  guides(fill = guide_legend(override.aes = list(size=5), order = 1))+  #changing size of legend circles of "Gun ownership" and placing "Gun ownership" legend above "White population"
  coord_fixed(ratio = 5) + #making the plot symmetric (square)
  annotate(geom="text", x=25, y=0.75, label="Spearman's rho: 0.74") #adding Spearman's rho correlation as in article graph

```


# Challenge 2: 2016 California Contributors plots

The objective of this challenge is to reproduce a plot depicting the top 10 cities with the maximum contributions to Ms. Clinton’s and Donald Trump’s campaign. The author of this code is clearly more biased towards one than the other.

The plot of the end-result will look like:

```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```

The first step involves loading the CA_contributors_2016.csv file into a dataframe.

```{r, load_CA_data, warnings= FALSE, message=FALSE}
# We will load our datasets with vroom because vroom vroom. 

CA_contributors_2016 <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))
glimpse(CA_contributors_2016)


```

We can see that the attributes of this dataframe contain the details of the contributions to each candidate: the name of the candidate receiving the contribution, the amount received, the ZIP code of the donator, and the contribution date.
Our task at hand involves finding the top 10 cities from where the contributions actually came from to Ms. Clinton and Donald, but we don’t have any details about the cities in this dataframe! So how do we solve this problem? In order to solve this problem, we will download the zip code database and join it with our dataframe to find the cities.

```{r, zip_code, warnings= FALSE, message=FALSE}
# Let's vroom vroom the zip code data and see a glimpse of it. 
zip_code_database <- vroom::vroom(here::here("data","zip_code_database.csv"), col_types = cols(zip = col_double()))
glimpse(zip_code_database)
```

The attribute that is common in both tables is the one which we will use our join function on! We will be using an inner join to join the tables because we only want those records that lie in the intersection of the tables. Now let’s actually create the dataframe that we need for our plot!

```{r, create_dataframe, warnings= FALSE, message=FALSE, echo=TRUE}

CA_plot_data <- CA_contributors_2016 %>% 
  # We will begin by inner joining the table with the zip code table.
  inner_join(zip_code_database) %>% 
  # There are 19 attributes in the resultant dataframe, obtained after joining the two dataframes. We will select only the 3 we actually need.
  select(cand_nm, contb_receipt_amt, primary_city) %>% 
  #We will group by candidate name and the primary city attribute
  group_by(cand_nm, primary_city) %>% 
  # Only those records with Trump and Ms. Clinton are needed
  filter(cand_nm == "Clinton, Hillary Rodham" | cand_nm == "Trump, Donald J.") %>% 
  # The total_sum attribute will now contain the total sum collected in each city by both of the candidates.  
  summarise(total_per_city = sum(contb_receipt_amt)) %>% 
  # Now, we select only the top 10 cities
  top_n(10, total_per_city) %>% 
  # Now, we need to reorder the total_per_city to make sure that it is in descending order! 
  ungroup() %>%
  mutate(cand_nm = as.factor(cand_nm), primary_city = reorder_within(primary_city, total_per_city, cand_nm)) 

CA_plot_data
```
Now that we have all of the data that we actually need to construct the plot, it’s time to get our hands dirty! After taking a look at the dataframe we can clearly see that San Frannsico and LA donated crazy amounts of money to Ms. Clinton’s campaign. The highest amount she received was a staggering USD 1.23 million, whereas Trump’s highest amount was just about USD 0.54 million. I guess that this result would lead to the hypothesis - “Money can buy you everything” being proved false haha. But of course, jokes aside, the real numbers that would be important would be total contributions from all cities and not just the total contribution per city.

Replication:
```{r, replication, warnings= FALSE, message=FALSE, echo=TRUE, fig.width=9, fig.height=4.5}
# The magnitudes of the donations to Trump's and Ms. Clinton's campaigns are clearly quite different and therefore, we will need to use different scales on the x-axis! 
x_scales <- function(x) { 
  if (max(x) < 600000) 
    c(0, 200000, 400000) 
  else 
    c(0, 4000000, 8000000, 12000000) }

# Getting our hands dirty with plots
ggplot(CA_plot_data, aes(x = primary_city, y = total_per_city, fill = cand_nm)) + 
  #We need to plot a bar chart - but we don't need a legend
  geom_col(show.legend = FALSE) + 
  # We need bar charts for both the candidates! 
  facet_wrap(~cand_nm, scales = "free") + 
  # We want the x-axis to hold the total contribution amounts and y-axis to hold the city names
  coord_flip() +
  # Reoordering the x-axis
  scale_x_reordered() +
  # We need continuoius position scales for our y-values
  scale_y_continuous(labels = scales::dollar, breaks = x_scales) + 
  # Making tweaks to the presentation of the graph
  # Adding 
  labs(subtitle = "Where did candidates raise most money?",
       x = NULL,
       y = "Amount raised") + 
  # Changing the colours
  scale_fill_manual(values=c("#2e74c0", "#cb454a")) + 
  theme(
    # Changing the subtitle size
    plot.subtitle = element_text(size = 10), 
    # Changing Axes Title Sizes
    axis.title = element_text(size = 9),
    # Changing Axes Text Sizes
    axis.text=element_text(size=7), 
    # Adding grey grid lines (77 is a lucky number!) 
    panel.grid.major = element_line(colour = "gray77"), 
    panel.grid.minor = element_line(colour = "gray77"), 
    # Adding a frame around the plots
    panel.background = element_rect(color="black", size=0.5, fill = NA), 
    strip.text = element_text(size=8),
  )

```
Voila! Now, we have replicated the plot that depicts the top 10 cities that contributed to each of the two candidates. This task was pretty challenging because it can get a little annoying to exactly duplicate the styling… but the end result looks great! :)

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

# Details

- Who did you collaborate with: Lexin Wu, Ozlem Cuhaci, Advait Jayant, Rasul Rasulov, Joseph Perrin 
- Approximately how much time did you spend on this problem set: Around 40 hours (Counting the individual contribution times separately and the Zoom Group contribution times as one)
- What, if anything, gave you the most trouble: Replicating the Challenges indeed turned out to be a challenge because we had to replicate exactly the fonts, colours, text sizes, and little things like that. Documenting the code also took an insaaaaaaane amount of time as well. Our objective while preparing this project wasn't just answering the questions but actually using the datasets and our analysis to tell a story on them. This is what we have tried to reflect in each of the tasks, but since we are still learning this process, we expect to improve over the course of the programme so please excuse us if the stories aren't as elaborate as they could have been. :)  

# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.










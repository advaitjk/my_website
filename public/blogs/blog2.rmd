---
categories:
- ""
- ""
date: "2017-10-31T22:26:13-05:00"
description: 
draft: false
image: pic02.jpg
keywords: "trump"
slug: blog2
title: Trump's Approval Ratings  
---
# Trump's Approval Margins

<center><img src="https://d.newsweek.com/en/full/607858/adsgads.jpg"></center>

Objective: The primary objective of this part of the challenge is to clearly find how Trump's approval ratings fluctuated as a function of time (in weeks) throughout the past 4 years. 
Dataset Used: Five-Thirty-Eight.com (Trump-Approval Data)

``` {r include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(scales)
library(dplyr)
library(kableExtra)
```


## Part-1: Reproducing the Plot
Our end-goal is to be able to reproduce the following plot: 
```{r trump_margins, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)
```

Let us begin the project by importing the CSV file containing the data of the approval poll into our document.

``` {r importingcsv}
# Importing the Approval Rating into a DataFrame
approval_polllist <- vroom::vroom(here::here('data', 'approval_polllist.csv'))
glimpse(approval_polllist)
```

### Data Wrangling

As Data Scientists, it is our aim and mission to make sure that we follow all the Tidy Data protocols and handle clean data in our approach. 

``` {r data_wrangling}
tidy_approvals <- approval_polllist %>% 
  # Filtering out all of the voters in the dataset
  filter(subgroup %in% "Voters") %>% 
  # Selecting all of the necessary attributes; 
  # We will use the poll_id attribute to check for duplicates
  select(enddate, approve, disapprove, poll_id) 

# Checking for duplicates and missing entries
any(duplicated(tidy_approvals))
```
We seem to have dealt with all of the duplicates in our dataset at this point. Let us take a look at our dataframe and check for missing values.

``` {r missing_values_check}
any(is.na(tidy_approvals))
```

Let us now glimpse into the dataframe.

``` {r glimpsing}
glimpse(tidy_approvals)
```

Now that we can see that there are no missing values or duplicates in our dataset; we can now perform data transformations to sort the data by data and group the data by week and year in order to extract the confidence intervals! 

``` {r data_transformations}
grouped_approvals <- tidy_approvals %>% 
  # We need to extract the year and week numbers from the data
  mutate(date = mdy(enddate), 
         weeknr = week(date), 
         yearnr = year(date)) %>%
  
  # Now we will group the data by week number and year number
  group_by(weeknr, yearnr) %>%
  
  # The next step is to summarise the average net approval ratings, standard deviation, 
  # count, and t-critical values
  summarise(avg_net_apr = mean(approve - disapprove), 
            std = sd(approve - disapprove), 
            count = n(), 
            t_crit = qt(.975, count-1)) %>% 
  # We will create a Standard Error attribute using the formula 
  #(Standard Error = Standard Deviation/Square-root(Count))
  
  mutate(se = std / sqrt(count)) %>% 
  
  # Now, we will calculate the bounds of the confidence interval
  mutate(upper = avg_net_apr + t_crit*se,
          lower = avg_net_apr - t_crit*se) 
  
grouped_approvals
```

### Constructing the Plot

``` {r fun_plot, out.width="100%"}

grouped_approvals %>%
  ggplot(aes(weeknr, avg_net_apr))+
  # We will facet wrap the plot by year
  facet_wrap(~yearnr)+
  # We will now add a line according to the points and color the plot by yearnr
  geom_path(aes(color = factor(yearnr))) + geom_point(aes(color = factor(yearnr)), size = 0.85) +
  # Displaying the confidence interval
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = factor(yearnr), color = factor(yearnr)), alpha = 0.1)+ 
  # We will create the orange horizontal line with the y-intercept = 0
  geom_hline(yintercept=0, color = "orange")+
  # Scaling the two axis
                     # We will add a border around the plots
  scale_x_continuous(expand = expansion(mult = .07), 
                     # Let us now construct the breaks on the x-axis
                     breaks=seq(0,52,13)) + 
  # Similarly, we want 12 breaks on the y-axis
  scale_y_continuous(breaks = breaks_extended(n=12)) + 
  # We will use the black and white theme
  theme_bw() + 
  # We will add a black border around the plots
  theme(aspect.ratio = 1/3, legend.position = "none") + 
  # Labelling the plot exactly as it is in the image
  labs(x = "Week of the year", 
       y = "Averge Net Approval (%)", 
       title = "Trump's popularity remained steadily below 50% throughout his term", 
       subtitle = "Estimated Net Approval (approve-disapprove) for Donald Trump")
```

What the plot should have actually looked like: 
```{r trump_margins2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)
```

The two plots are nearly almost identical, however, it is easy to observe that there exists a clear differnce in the confidence interval. In the plot that we have to reproduce, the confidence interval was obtained by a multiplication of 1.96, but we think that using the t-critical value may be a better approach to solving this problem. 

In the plot, we can created, we can easily visualise that in 2017, Trump's approval rating declined quite rapidly!! However, since then Trump has managed to retain his approval ratings of around 40% without having too much trouble. 
In 2020, we can see that the early weeks clearly demonstrate that his approval ratings started to decline, probably because of the Coronavirus crisis and the way he handled it. The rise of the BLM movement may also have played a role in the decline in approval ratings. We have a feeling that it was actually a combination of the lockdowns imposed in the US and the BLM movement that led to this decline in approval ratings. However, it seems that as the 2020 presidential election grows closer, Trump's rating has started to go back on the rise! Let's see if he truly does come out as a victor in this political championship again. 
Let us now try to compare the confidence intervals.

### Comparing the Confidence Intervals

Our next objective is to compare the confidence intervals for weeks 15 (6-12 April 2020) and 34 (17-23 August 2020).

``` {r confidence_15}
grouped_approvals %>% 
  filter(weeknr == 15, yearnr == 2020) %>% 
  select(lower, upper)
```

``` {r confidence_34}
grouped_approvals %>% 
  filter(weeknr == 34, yearnr == 2020) %>% 
  select(lower, upper)
```
We can clearly see that the confidence intervals for the two weeks are quite different in the sense that the confidence interval for week 34 is far higher than for week 15. The difference may have arisen from the difference in standard deviations: the standard deviation for week 15 is considerably lesser than that for week 34. The other reason is probably because of the higher sample size. As the sample size  increases, the deviation from the central tendency tends to decrease and therefore, this leads to smaller confidence intervals.

Voila! Now, we have replicated the plot that depicts the top 10 cities that contributed to each of the two candidates. This task was pretty challenging because it can get a little annoying to exactly duplicate the stylingâ€¦ but the end result looks great! :)
